# ü§ñ Seguran√ßa de Intelig√™ncia Artificial (AI Security)

Este documento descreve os **fundamentos, riscos, controles e carreiras** relacionados √† **seguran√ßa de sistemas de Intelig√™ncia Artificial**, incluindo **Machine Learning (ML), LLMs, GenAI e AI-enabled systems**.

Seguran√ßa de IA **n√£o √© uma disciplina isolada**.  
Ela √© uma **extens√£o natural de AppSec, Data Security, Cloud, DevSecOps e GRC**, com **novas superf√≠cies de ataque**.

---

## üéØ Objetivos da Trilha

- Entender **como sistemas de IA realmente funcionam**
- Identificar **vetores reais de ataque (n√£o te√≥ricos)**
- Aplicar **controles t√©cnicos e organizacionais**
- Integrar seguran√ßa de IA ao **SDLC, DevSecOps e GRC**
- Preparar profissionais para **AI Security e AI Governance**

---

## üß± Fundamentos T√©cnicos de IA (Obrigat√≥rios)

### Conceitos de IA e ML
- Artificial Intelligence (AI)
- Machine Learning (ML)
- Deep Learning
- Modelos supervisionados, n√£o supervisionados e por refor√ßo
- Pipeline de ML: coleta ‚Üí treinamento ‚Üí valida√ß√£o ‚Üí infer√™ncia
- Overfitting, underfitting e data leakage

Conte√∫dos base:
- https://developers.google.com/machine-learning/crash-course
- https://www.coursera.org/learn/machine-learning

---

### LLMs e GenAI
- Large Language Models (LLMs)
- Tokens, contexto e embeddings
- Prompting e prompt chaining
- Fine-tuning vs **RAG (Retrieval-Augmented Generation)**
- Modelos fechados vs open source
- APIs de infer√™ncia e agentes

Conte√∫dos:
- https://platform.openai.com/docs
- https://huggingface.co/docs
- https://lilianweng.github.io/posts/2023-06-23-agent/

---

## üß® Principais Amea√ßas em Seguran√ßa de IA

### Ataques ao Modelo
- **Prompt Injection**
- **Jailbreak de LLM**
- Model Inversion
- Model Extraction
- Membership Inference

### Ataques aos Dados
- **Data Poisoning**
- Training data leakage
- Dataset bias intencional
- Manipula√ß√£o de datasets externos (RAG)

### Ataques √† Infraestrutura de IA
- Comprometimento de pipelines de ML
- Abuso de APIs de infer√™ncia
- Exposi√ß√£o de tokens e chaves
- Falhas de IAM em servi√ßos de IA
- Escalada via plugins/agentes

### Ataques de Uso Indevido
- Gera√ß√£o de malware
- Phishing e engenharia social em escala
- Abuso de automa√ß√µes baseadas em IA
- Bypass de controles humanos

---

## üß© Superf√≠cies de Ataque em Sistemas de IA

- Prompts e entradas do usu√°rio
- APIs de infer√™ncia
- Pipelines de dados
- Ambientes de treinamento
- Artefatos de modelo (weights, checkpoints)
- Integra√ß√µes com sistemas corporativos
- Plugins, agentes e ferramentas externas

---

## üõ°Ô∏è Controles de Seguran√ßa para IA

### üîß Controles T√©cnicos
- Valida√ß√£o e sanitiza√ß√£o de prompts
- Rate limiting e autentica√ß√£o forte
- Isolamento de ambientes (train / test / prod)
- Monitoramento de infer√™ncia
- Logging e auditoria de prompts e respostas
- Prote√ß√£o de modelos e artefatos
- Sandbox / containers para execu√ß√£o

---

### üß™ Controles de Aplica√ß√£o (Secure AI SDLC)
- Threat Modeling espec√≠fico para IA
- Testes de seguran√ßa em prompts
- Red Teaming de IA
- Guardrails de entrada e sa√≠da
- Human-in-the-loop para decis√µes cr√≠ticas

---

### üß¨ Controles de Dados
- Classifica√ß√£o e rotulagem de dados
- Minimiza√ß√£o de dados
- Mascaramento e anonimiza√ß√£o
- Controle de acesso a datasets
- Auditoria de datasets de treinamento

---

## üß† Frameworks e Refer√™ncias T√©cnicas

### Frameworks Oficiais
- **NIST AI Risk Management Framework (AI RMF)**  
  https://www.nist.gov/itl/ai-risk-management-framework

- **OWASP Top 10 for LLM Applications**  
  https://owasp.org/www-project-top-10-for-large-language-model-applications/

- **MITRE ATLAS ‚Äì Adversarial Threat Landscape for AI Systems**  
  https://atlas.mitre.org/

- **ISO/IEC 23894 ‚Äì AI Risk Management**  
  https://www.iso.org/standard/77304.html

- **ISO/IEC 27001 / 27701** (controles transversais)

---

## üß∞ Ferramentas Open Source Importantes

### üîç AI / LLM Security
- **PromptFoo** ‚Äì https://github.com/promptfoo/promptfoo
- **Garak (LLM Vulnerability Scanner)** ‚Äì https://github.com/leondz/garak
- **LLM Guard** ‚Äì https://github.com/protectai/llm-guard
- **Rebuff (Prompt Injection Defense)** ‚Äì https://github.com/protectai/rebuff

---

### üß™ Red Teaming de IA
- **Microsoft PyRIT** ‚Äì https://github.com/Azure/PyRIT
- **OpenAI Evals (framework)** ‚Äì https://github.com/openai/evals
- **AI Red Teaming Resources (NIST)**  
  https://airc.nist.gov/

---

### üîê Data & Pipeline Security
- **MLflow** ‚Äì https://mlflow.org/
- **Great Expectations (Data Quality)** ‚Äì https://greatexpectations.io/
- **OpenLineage** ‚Äì https://openlineage.io/

---

## üß™ Labs Pr√°ticos (AI Security)

> Seguran√ßa de IA **se aprende explorando modelos reais**.

### Labs e Ambientes
- **OWASP LLM Security Labs**  
  https://github.com/OWASP/www-project-top-10-for-large-language-model-applications

- **Prompt Injection Playground**  
  https://github.com/evilrobot01/prompt-injection-playground

- **TryHackMe ‚Äì AI & LLM Rooms (em evolu√ß√£o)**  
  https://tryhackme.com/

- **HuggingFace Spaces (model testing)**  
  https://huggingface.co/spaces

---

## üîÅ Integra√ß√£o com DevSecOps e GRC

### DevSecOps
- Seguran√ßa desde o design
- Pipelines de ML seguros
- Versionamento de modelos
- Monitoramento cont√≠nuo de infer√™ncia
- Integra√ß√£o com CI/CD

---

### GRC & Conformidade
- Avalia√ß√£o de risco de IA
- Pol√≠ticas de uso aceit√°vel de IA
- Governan√ßa de modelos
- LGPD / GDPR e dados usados em IA
- Prepara√ß√£o para regula√ß√µes de IA (EU AI Act)

---

## üë• Carreiras em Seguran√ßa de IA

### Pap√©is T√©cnicos
- AI Security Engineer
- AppSec com foco em IA
- ML Engineer com foco em seguran√ßa
- AI Red Team / AI Blue Team

### Pap√©is de Governan√ßa
- AI Risk Analyst
- AI Governance Specialist
- Security Architect (AI-enabled systems)
- CISO respons√°vel por IA

---

## üéì Cursos e Capacita√ß√£o

### Cursos Oficiais
- **NIST AI RMF Training**  
  https://www.nist.gov/itl/ai-risk-management-framework

- **OWASP LLM Security Workshops**  
  https://owasp.org/

- **Microsoft ‚Äì Secure Generative AI**  
  https://learn.microsoft.com/security/engineering/secure-ai

- **Google ‚Äì Responsible AI**  
  https://ai.google/responsibility/

---

## üéì Certifica√ß√µes (Estado Atual do Mercado)

> Ainda **n√£o existe certifica√ß√£o √∫nica e madura** de AI Security.

Recomenda-se combinar:
- **AppSec (OSWE, CSSLP)**
- **Cloud Security (CCSP, AWS Security)**
- **GRC / Risk (CISSP, CRISC)**
- **Privacidade (CDPSE, ISO 27701)**

Certifica√ß√µes emergentes devem ser avaliadas com cautela.

---

## üìå Princ√≠pios-Chave de Seguran√ßa de IA

- IA √© software + dados + infra
- Prompt √© superf√≠cie de ataque
- Modelo sem governan√ßa vira risco
- Automa√ß√£o amplia impacto do erro
- Seguran√ßa de IA √© cont√≠nua, n√£o projeto

---
